{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5084efd8",
   "metadata": {},
   "source": [
    "# Miniproject: Deep Reinforcement Learning for Flappy Bird\n",
    "* MDP Formulation: $(S,A,P_{a},R_{a})$\n",
    "* Q-Learning and Deep Q-Learning\n",
    "* Used DQN architecture\n",
    "* Training and results\n",
    "* Conclusion and Outlook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc72c0",
   "metadata": {},
   "source": [
    "# Markov Decision Process Formulation\n",
    "### FlappyBird Environment\n",
    "* Goal: control the bird and stay alive (and beat the high score)\n",
    "* published in 2013\n",
    "* 50.000$ a day with in-app advertisement\n",
    "\n",
    "<video controls src=\"assets/test.mkv\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01be36",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "FlappyBird game using pygame platform\n",
    "\n",
    "FPS: 30 \n",
    "\n",
    "observation: current_screen, reward, terminal_state\n",
    "\n",
    "### Action definition:\n",
    "\n",
    "ActionSpace = A = {1 (flap), 0 (drop)}\n",
    "\n",
    "### State definition:\n",
    "\n",
    "$s_t = x_{t-histLen+1} ,\\; a_{t-histLen+1},\\; ... ,\\; x_{t-1},\\; a_{t-1},\\; x_t$\n",
    "\n",
    "with $x_t: $ pixel input\n",
    "\n",
    "$a_t: $ action taken\n",
    "\n",
    "$histLen$: number of stack of observations (temporal information)\n",
    "\n",
    "### Transitions:\n",
    "unknown\n",
    "\n",
    "### Reward:\n",
    "unknown\n",
    "\n",
    "### Ingame Reward Definitions\n",
    "\n",
    "| Name  | reward |\n",
    "|---:|:-------------|\n",
    "| rewardAlive | 0.1  | \n",
    "| rewardPipe |  1  | \n",
    "| rewardDead | -1  | \n",
    "\n",
    "$R_t = r_t + \\gamma * R_{t+1}$\n",
    "\n",
    "with $\\gamma$ as discount factor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99c1dc",
   "metadata": {},
   "source": [
    "# Q-Learning / Deep Q-Learning (DQN)\n",
    "\n",
    "### Q-Learning:\n",
    "* model-free (estimate the Q-function)\n",
    "* Q(s, a) represents the estimate of future rewards\n",
    "* find $\\pi^*$ by choosing action with highest q-value\n",
    "* off policy (finds $\\pi^*$ independent of behavioral policy)\n",
    "\n",
    "<img src=\"assets/q-learning_table.png\" width= \"800\"> Figure from [1] \n",
    "\n",
    "${Q^{\\pi^{*}}(s, a) = r + \\gamma * max_{a^{'}} Q(s^{'}, a^{'})}$\n",
    "\n",
    "Update Q-Values with temporal difference step:\n",
    "\n",
    "$Q(s, a)_{new} \\leftarrow Q(s, a)_{old} + \\alpha (y - Q(s, a)_{old})$\n",
    "\n",
    "with $y = r + \\gamma max_a^{'} Q(s^{'}, a^{'})_{old}$\n",
    "\n",
    "\n",
    "\n",
    "#### What if the state space is too large?\n",
    "- Go: $10^{170}$\n",
    "- continues state space in helicopter flying\n",
    "\n",
    "### Function approximator for Q-function\n",
    "$Q(s, a, \\textbf{w}) \\approx Q(s, a)$\n",
    "\n",
    "<img src=\"assets/dqn_network.png\" width=\"800\"> Figure from [1] \n",
    "\n",
    "$\\mathcal{L_i(\\mathbf{w_i})} = \\mathbb{E}_{s,a \\sim œÅ(s,a)}\\big[\\big(y -  Q(s, a, \\mathbf{w_i})\\big)^2 \\big] $\n",
    "\n",
    "$y_i = \\mathbb{E}_{s^{'} \\sim \\varepsilon} \\big[r + \\gamma * max_{a^{'}} Q(s^{'}, a^{'}, \\mathbf{w_{i}})\\big]$\n",
    "### Problem: \n",
    "* non-stationary target: changing the weights changes the target\n",
    "* trajectories are correlated (we want i.i.d.)\n",
    "\n",
    "$\\rightarrow$ congergence issues in training the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75686b",
   "metadata": {},
   "source": [
    "####  Mnih et al., 2013, Playing Atari with Deep Reinforcement Learning\n",
    "#### Mnih et al., 2015, Human-level control through deep reinforcement learning\n",
    "## Expierence replay\n",
    "tuple $expierence =  (s, a, r, s')$\n",
    "\n",
    "store in replay memory $\\mathcal{D}$\n",
    "\n",
    "* REPLAY_MEMORY = 50000 \n",
    "* sample a minibatch of all expieriences in the replay memory\n",
    "\n",
    "$\\rightarrow$ decorrelates the observations\n",
    "\n",
    "\n",
    "## Fixed target network\n",
    "* target and predictor network are the same but with different weights\n",
    "* freeze the target for c-th iterations\n",
    "* assign current weights to target network\n",
    "\n",
    "\n",
    "$\\mathcal{L_i(\\mathbf{w_i})} = \\mathbb{E}_{s,a,r,s^{'} \\sim \\mathcal{D}}\\big[\\big(r + \\gamma * max_{a^{'}} Q(s^{'}, a^{'}, \\mathbf{w_{i-c}}) -  Q(s, a, \\mathbf{w_i})\\big)^2 \\big] $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04368212",
   "metadata": {},
   "source": [
    "# Used DQN architecture\n",
    "\n",
    "\n",
    "## Image Preprocessing\n",
    "* set background to black\n",
    "<img src=\"assets/imgProcess_diagramm.png\">\n",
    "<img src=\"assets/preprocessing.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54716bc5",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Trainable weights: 3,356,322\n",
    "<img src=\"assets/model.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b33b6",
   "metadata": {},
   "source": [
    "## Game difficulties\n",
    "\n",
    "| difficulty  | pipeGapSize |\n",
    "|---:|:-------------|\n",
    "| easy | 200   | \n",
    "| medium | 150   | \n",
    "| hard | 100 | \n",
    "\n",
    "<img src=\"assets/pipeSize.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff7465",
   "metadata": {},
   "source": [
    "# Explorations vs Exploitation: Epsilon-greedy \n",
    "* decay epsilon over time\n",
    "\n",
    "* generaly start with INITIAL_EPSILON = 1.0\n",
    "* agent can choose an action every 33ms (FPS=30): \n",
    " * action \"flap\" changes accleration by value of -9\n",
    " * action \"do nothing\" changes accleration by value of 1\n",
    "\n",
    "\n",
    "<img src=\"assets/epsilon.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7928b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "* arround 15-17h on a GPU\n",
    "* start training after 1000 obervations\n",
    "* loss: Mean squared error\n",
    "* optimizer= Adam\n",
    "* batchsize = 32\n",
    "* decay rate $\\gamma$ = 0.95 \n",
    "* learning rate = $10^{-4}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0854d27",
   "metadata": {},
   "source": [
    "## Learning plots for \"easy\"\n",
    "\n",
    "<img src=\"assets/learning_plots.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43902fbe",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Mean score of DQN for different difficulties and different learning iterations\n",
    "\n",
    "* score results are based on the mean of 10 games with epsilon = 0\n",
    "* terminate game after score passed 1000 points which is regarded as infinity (inf)\n",
    "\n",
    "\n",
    "| # iterations  | easy | medium | hard |\n",
    "|---:|:-------------|-------|-------|\n",
    "| 990000 | inf  |  231.2  |  18.3  | \n",
    "| 199000 | 930.3   | 913.9  |  22.5  | \n",
    "| 299000 | 223.8   |  105.4  |  197.7  | \n",
    "| 399000 |  795.1  |  21.0  |  91.7  | \n",
    "| 499000 |  275.5  |  14.4 |  30.7  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65cd79",
   "metadata": {},
   "source": [
    "# How good are the trained models for the other difficulties?\n",
    "\n",
    "### trained on \"easy\"\n",
    "\n",
    "| # iterations  | easy | medium | hard |\n",
    "|---:|:-------------|-------|-------|\n",
    "| 990000 | inf  |  10.9  |  0.3  | \n",
    "| 199000 | 930.3   | 1.7  |  0.0  | \n",
    "| 299000 | 223.8   |  7.3  |  0.2  | \n",
    "| 399000 |  795.1  |  1.1  |  0.2  | \n",
    "| 499000 |  275.5  |  4.1 |  0.1  |\n",
    "\n",
    "### trained on \"medium\"\n",
    "\n",
    "| # iterations  | easy | medium | hard |\n",
    "|---:|:-------------|-------|-------|\n",
    "| 990000 | inf  |  231.2  |  2.3  | \n",
    "| 199000 | 133.2   | 913.9  |  1.2  | \n",
    "| 299000 | inf  |  105.4  |  0.5  | \n",
    "| 399000 |  682.4  |  21.0  |  0.9  | \n",
    "| 499000 |  62.4  |  14.4 |  0.3  |\n",
    "\n",
    "\n",
    "\n",
    "### trained on \"hard\"\n",
    "\n",
    "| # iterations  | easy | medium | hard |\n",
    "|---:|:-------------|-------|-------|\n",
    "| 990000 | 6.8  |  4.2  |  18.3  |\n",
    "| 199000 | 233.3   | 64.7  |  22.5  | \n",
    "| 299000 | 57.4  |  287.4  |  197.7  | \n",
    "| 399000 |  156.8  |  165.1  |  91.7  | \n",
    "| 499000 |  23.5  |  50.8 |  30.7  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24318ce5",
   "metadata": {},
   "source": [
    "## Comparing Network with and without Fixed Target Network\n",
    "##### not fixed target network\n",
    "<img src=\"assets/scores_10k.png\">\n",
    "\n",
    "##### fixed target network\n",
    "<img src=\"assets/fixedTarget_hard_scores_10k.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb93e6",
   "metadata": {},
   "source": [
    "## Comparing Training with and without rewardAlive\n",
    "\n",
    "| # iterations  | easy | easy with rewardAlive | medium | medium with rewardAlive | hard | hard with rewardAlive |\n",
    "|---:|:-------------|-------|-------|-----|-------|-----|\n",
    "| 990000 | 999.5   |  <strong>inf</strong>  |  95.0  | <strong>231.2</strong>  | 3.1  | <strong>18.3</strong>\n",
    "| 199000 | 55.0   |  <strong>930.3</strong>  |  7.1  | <strong>913.9</strong>  | 9.6  | <strong>22.5</strong>\n",
    "| 299000 | 47.3  |  <strong>223.8</strong>  |  20.6 | <strong>105.4</strong>  | 26.3 | <strong>197.7</strong>\n",
    "| 399000 | 43.5  |  <strong>795.1</strong>  |  <strong>85.0</strong>  | 21.0  | <strong>116.5</strong> | 91.7\n",
    "| 499000 |  87.5  |  <strong>275.5</strong>  |  <strong>74.7</strong>  | 14.4  | 24.9 | <strong>30.7</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1118b84",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "* DQN is working for different difficulties and is reaching high scores (better as human player)\n",
    "* 500.000 learning steps are not necessary\n",
    "* training is \"stable\" witout fixed target network\n",
    "* rewardAlive gives a slight advantage (no sparse rewards)\n",
    "\n",
    "\n",
    "* Same architecture can be used for different games\n",
    "\n",
    "\n",
    "## Problems and Outlook\n",
    "* training is not consistent (more training does not correlate with better score)\n",
    " * catastrophic forgetting\n",
    " * save the network parameters that resulted in the best test performance\n",
    " * clipping the gradient between ‚àí1.0 and 1.0 (no drastic weight changes by single mini-batch)\n",
    "\n",
    "\n",
    "* minibatches are sampled uniformly from replay buffer \n",
    " * Prioritized Experience Replay (pay more attention to samples with large target gap)\n",
    "\n",
    "\n",
    "\n",
    "* explore fixed target network\n",
    "* use of Dueling DQN Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa64fb6",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
